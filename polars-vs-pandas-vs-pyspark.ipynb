{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2194131,"sourceType":"datasetVersion","datasetId":684488}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install polars","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-24T03:08:18.979069Z","iopub.execute_input":"2024-09-24T03:08:18.979892Z","iopub.status.idle":"2024-09-24T03:08:34.330072Z","shell.execute_reply.started":"2024-09-24T03:08:18.979843Z","shell.execute_reply":"2024-09-24T03:08:34.328670Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: polars in /opt/conda/lib/python3.10/site-packages (1.7.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-09-24T03:12:04.809789Z","iopub.execute_input":"2024-09-24T03:12:04.810260Z","iopub.status.idle":"2024-09-24T03:12:05.592161Z","shell.execute_reply.started":"2024-09-24T03:12:04.810196Z","shell.execute_reply":"2024-09-24T03:12:05.591178Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 1. Prueba de Carga de Datos (Tiempo de Ejecución)","metadata":{}},{"cell_type":"code","source":"# Medir el tiempo de carga con pandas\nstart_time = time.time()\ndf_pandas = pd.read_csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\")\npandas_time = time.time() - start_time\nprint(f\"Tiempo de carga con pandas: {pandas_time:.4f} segundos\")","metadata":{"execution":{"iopub.status.busy":"2024-09-24T03:12:11.510505Z","iopub.execute_input":"2024-09-24T03:12:11.511077Z","iopub.status.idle":"2024-09-24T03:14:33.112296Z","shell.execute_reply.started":"2024-09-24T03:12:11.511036Z","shell.execute_reply":"2024-09-24T03:14:33.110747Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Tiempo de carga con pandas: 141.5910 segundos\n","output_type":"stream"}]},{"cell_type":"code","source":"# Medir el tiempo de carga con polars\nstart_time = time.time()\ndf_polars = pl.read_csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\")\npolars_time = time.time() - start_time\nprint(f\"Tiempo de carga con polars: {polars_time:.4f} segundos\")","metadata":{"execution":{"iopub.status.busy":"2024-09-24T03:15:03.636955Z","iopub.execute_input":"2024-09-24T03:15:03.637477Z","iopub.status.idle":"2024-09-24T03:15:24.403398Z","shell.execute_reply.started":"2024-09-24T03:15:03.637429Z","shell.execute_reply":"2024-09-24T03:15:24.402143Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Tiempo de carga con polars: 20.7594 segundos\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Prueba de Uso de Memoria (Memory Profiler)","metadata":{}},{"cell_type":"code","source":"from memory_profiler import memory_usage\n\ndef load_pandas():\n    return pd.read_csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\")\n\ndef load_polars():\n    return pl.read_csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\")\n\n# Medir uso de memoria con pandas\npandas_mem = memory_usage(load_pandas)\nprint(f\"Uso de memoria con pandas: {max(pandas_mem) - min(pandas_mem):.4f} MB\")\n\n# Medir uso de memoria con polars\npolars_mem = memory_usage(load_polars)\nprint(f\"Uso de memoria con polars: {max(polars_mem) - min(polars_mem):.4f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-09-24T03:17:25.091762Z","iopub.execute_input":"2024-09-24T03:17:25.092270Z","iopub.status.idle":"2024-09-24T03:19:53.151035Z","shell.execute_reply.started":"2024-09-24T03:17:25.092201Z","shell.execute_reply":"2024-09-24T03:19:53.149418Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Uso de memoria con pandas: 13805.8203 MB\nUso de memoria con polars: 11661.8516 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Monitoreo de CPU y Memoria con psutil","metadata":{}},{"cell_type":"code","source":"import psutil\n\ndef monitor_performance(func):\n    process = psutil.Process()\n    start_cpu = process.cpu_percent(interval=None)\n    start_mem = process.memory_info().rss / (1024 * 1024)  # Convertir a MB\n    start_time = time.time()\n\n    # Ejecutar la función\n    func()\n\n    elapsed_time = time.time() - start_time\n    end_cpu = process.cpu_percent(interval=None)\n    end_mem = process.memory_info().rss / (1024 * 1024)  # Convertir a MB\n\n    print(f\"Tiempo de ejecución: {elapsed_time:.4f} segundos\")\n    print(f\"CPU usada: {end_cpu - start_cpu:.2f}%\")\n    print(f\"Memoria usada: {end_mem - start_mem:.4f} MB\")\n\n# Monitoreo con pandas\nmonitor_performance(lambda: pd.read_csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\"))\n\n# Monitoreo con polars\nmonitor_performance(lambda: pl.read_csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-24T03:24:45.538560Z","iopub.execute_input":"2024-09-24T03:24:45.539566Z","iopub.status.idle":"2024-09-24T03:27:12.391362Z","shell.execute_reply.started":"2024-09-24T03:24:45.539509Z","shell.execute_reply":"2024-09-24T03:27:12.389903Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Tiempo de ejecución: 114.1603 segundos\nCPU usada: 99.90%\nMemoria usada: -1.2461 MB\nTiempo de ejecución: 32.6770 segundos\nCPU usada: 177.10%\nMemoria usada: 1.2969 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4. PySpark vs Polars","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-09-24T15:24:44.366048Z","iopub.execute_input":"2024-09-24T15:24:44.366459Z","iopub.status.idle":"2024-09-24T15:25:34.936415Z","shell.execute_reply.started":"2024-09-24T15:24:44.366420Z","shell.execute_reply":"2024-09-24T15:25:34.935001Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=7179957227795ba0961b30746d6f183208a6bc1649efe36ff439289987946596\n  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importar librerías necesarias\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport polars as pl\nimport time\n\n# --- Configuración de PySpark ---\n\n# Crear una sesión de Spark\nspark = SparkSession.builder \\\n    .appName(\"CSV-Data-Processing\") \\\n    .getOrCreate()\n\n# Leer datos desde el archivo CSV\nstart_time_read_spark = time.time()\ndf_spark = spark.read.csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\", header=True, inferSchema=True)\nend_time_read_spark = time.time()\n\n# Mostrar el tiempo de lectura en PySpark\nprint(f\"Tiempo de lectura PySpark: {end_time_read_spark - start_time_read_spark} segundos\")\n\n# Asignar nombres de columnas descriptivos\ndf_spark = df_spark.toDF(\"col_1\", \"col_103\", \"col_100\", \"col_000000190\", \"col_0\", \"col_35843\", \"col_34353\", \"col_extra\")\n\n# Filtrar los datos en PySpark\nstart_time_filter_spark = time.time()\nfiltered_spark = df_spark.filter(col('col_1') > 1000)\nresult_spark = filtered_spark.groupBy('col_103').count()\nend_time_filter_spark = time.time()\n\n# Mostrar resultados en PySpark\nprint(\"Resultados PySpark:\")\nresult_spark.show()\nprint(f\"Tiempo de filtrado y agrupación en PySpark: {end_time_filter_spark - start_time_filter_spark} segundos\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T15:55:46.608362Z","iopub.execute_input":"2024-09-24T15:55:46.608996Z","iopub.status.idle":"2024-09-24T15:59:49.743841Z","shell.execute_reply.started":"2024-09-24T15:55:46.608952Z","shell.execute_reply":"2024-09-24T15:59:49.742456Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Tiempo de lectura PySpark: 148.87319493293762 segundos\nResultados PySpark:\n","output_type":"stream"},{"name":"stderr","text":"[Stage 24:======================================================> (33 + 1) / 34]\r","output_type":"stream"},{"name":"stdout","text":"+-------+--------+\n|col_103|   count|\n+-------+--------+\n|      1|69088367|\n|      2|44518954|\n+-------+--------+\n\nTiempo de filtrado y agrupación en PySpark: 0.024688243865966797 segundos\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"import polars as pl\nimport time\n\n# --- Configuración de Polars ---\n\n# Leer datos desde el archivo CSV en Polars\nstart_time_read_polars = time.time()\ndf_polars = pl.read_csv(\"/kaggle/input/100-million-data-csv/custom_1988_2020.csv\")\nend_time_read_polars = time.time()\n\n# Mostrar el tiempo de lectura en Polars\nprint(f\"Tiempo de lectura Polars: {end_time_read_polars - start_time_read_polars} segundos\")\n\n# Asignar nombres de columnas descriptivos en Polars\ndf_polars = df_polars.rename({\n    \"1\": \"col_1\",\n    \"103\": \"col_103\",\n    \"100\": \"col_100\",\n    \"000000190\": \"col_000000190\",\n    \"0\": \"col_0\",\n    \"35843\": \"col_35843\",\n    \"34353\": \"col_34353\"\n})\n\n# Filtrar los datos en Polars\nstart_time_filter_polars = time.time()\nfiltered_polars = df_polars.filter(pl.col('col_1') > 1000)\n\n# Agrupar y contar en Polars (corrigiendo el método)\nresult_polars = filtered_polars.groupby('col_103').agg(pl.count().alias('count'))\nend_time_filter_polars = time.time()\n\n# Mostrar resultados en Polars\nprint(\"Resultados Polars:\")\nprint(result_polars)\nprint(f\"Tiempo de filtrado y agrupación en Polars: {end_time_filter_polars - start_time_filter_polars} segundos\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T16:02:26.705386Z","iopub.execute_input":"2024-09-24T16:02:26.705837Z","iopub.status.idle":"2024-09-24T16:02:43.797271Z","shell.execute_reply.started":"2024-09-24T16:02:26.705796Z","shell.execute_reply":"2024-09-24T16:02:43.795660Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Tiempo de lectura Polars: 17.000966787338257 segundos\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m filtered_polars \u001b[38;5;241m=\u001b[39m df_polars\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol_1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Agrupar y contar en Polars (corrigiendo el método)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m result_polars \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_polars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol_103\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(pl\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     31\u001b[0m end_time_filter_polars \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Mostrar resultados en Polars\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'groupby'"],"ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'groupby'","output_type":"error"}]},{"cell_type":"code","source":"# Celda 1: PySpark para Extraer Datos de BigQuery\n\nfrom pyspark.sql import SparkSession\nimport time\n\n# Crear una sesión de Spark\nspark = SparkSession.builder \\\n    .appName(\"BigQuery to DataFrame\") \\\n    .config(\"spark.sql.extensions\", \"com.google.cloud.spark.bigquery.BigQuerySparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_bigquery\", \"com.google.cloud.spark.bigquery.BigQueryCatalog\") \\\n    .getOrCreate()\n\n# Medir el tiempo de lectura desde BigQuery\nstart_time_read_spark = time.time()\n\n# Leer datos desde BigQuery\nproject_id = \"tu_proyecto_id\"\ndataset_id = \"tu_dataset_id\"\ntable_id = \"tu_tabla_id\"\ntable_name = f\"{project_id}.{dataset_id}.{table_id}\"\n\ndf_spark = spark.read \\\n    .format(\"bigquery\") \\\n    .option(\"table\", table_name) \\\n    .load()\n\nend_time_read_spark = time.time()\n\n# Mostrar el tiempo de lectura en PySpark\nprint(f\"Tiempo de lectura PySpark: {end_time_read_spark - start_time_read_spark} segundos\")\n\n# Mostrar el esquema y los datos\ndf_spark.printSchema()\ndf_spark.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Celda 2: Polars para Extraer Datos de BigQuery\n\nimport pandas as pd\nimport polars as pl\nimport time\n\n# Medir el tiempo de lectura desde BigQuery\nstart_time_read_polars = time.time()\n\n# Leer datos desde BigQuery\nproject_id = \"tu_proyecto_id\"\nquery = \"SELECT * FROM tu_dataset_id.tu_tabla_id\"\n\ndf_pandas = pd.read_gbq(query, project_id=project_id)\n\nend_time_read_polars = time.time()\n\n# Convertir a DataFrame de Polars\ndf_polars = pl.from_pandas(df_pandas)\n\n# Mostrar el tiempo de lectura en Polars\nprint(f\"Tiempo de lectura Polars: {end_time_read_polars - start_time_read_polars} segundos\")\n\n# Mostrar el esquema y los datos\nprint(df_polars)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark pandas-gbq polars google-cloud-bigquery\n","metadata":{},"execution_count":null,"outputs":[]}]}